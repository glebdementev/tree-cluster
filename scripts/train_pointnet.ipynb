{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## PointNet training on LAS cylinder point clouds\n",
        "\n",
        "This notebook trains a compact PointNet classifier on per-tree cylinder LAS files.\n",
        "\n",
        "Sections:\n",
        "- Imports and environment\n",
        "- Configuration\n",
        "- Utilities (file scan, stats)\n",
        "- Dataset (sampling/normalization)\n",
        "- Model (PointNetTiny)\n",
        "- Data preparation (labels/splits/balancing)\n",
        "- Training loop with live plots\n",
        "- Save artifacts (weights and species index)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "import json\n",
        "import math\n",
        "import random\n",
        "from dataclasses import dataclass\n",
        "from pathlib import Path\n",
        "from typing import Dict, Iterable, List, Optional, Tuple\n",
        "\n",
        "import laspy\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import clear_output, display\n",
        "\n",
        "# Reproducibility\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@dataclass(frozen=True)\n",
        "class TrainConfig:\n",
        "    repo_root: Path\n",
        "    data_root: Path\n",
        "    output_dir: Path\n",
        "    points_per_sample: int\n",
        "    batch_size: int\n",
        "    epochs: int\n",
        "    learning_rate: float\n",
        "    train_split: float\n",
        "    seed: int\n",
        "    num_workers: int\n",
        "    live_plot: bool\n",
        "\n",
        "# Assume this notebook lives in `scripts/`. Adjust if moved.\n",
        "repo_root = Path.cwd().parent\n",
        "data_root = repo_root / \"las_cylinders_5m\"\n",
        "output_dir = repo_root / \"models\"\n",
        "output_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "cfg = TrainConfig(\n",
        "    repo_root=repo_root,\n",
        "    data_root=data_root,\n",
        "    output_dir=output_dir,\n",
        "    points_per_sample=512,\n",
        "    batch_size=16,\n",
        "    epochs=25,\n",
        "    learning_rate=1e-3,\n",
        "    train_split=0.85,\n",
        "    seed=SEED,\n",
        "    num_workers=0,\n",
        "    live_plot=True,\n",
        ")\n",
        "\n",
        "cfg\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Utilities\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def find_all_las_files(root: Path) -> List[Path]:\n",
        "    return [p for p in root.rglob(\"*.las\")]\n",
        "\n",
        "\n",
        "def extract_species_from_filename(path: Path) -> str:\n",
        "    stem = path.stem\n",
        "    parts = stem.split(\"_\")\n",
        "    if len(parts) < 3:\n",
        "        return \"unknown\"\n",
        "    return parts[2]\n",
        "\n",
        "\n",
        "def compute_point_counts(files: List[Path]) -> List[int]:\n",
        "    counts: List[int] = []\n",
        "    for p in files:\n",
        "        las = laspy.read(p)\n",
        "        counts.append(len(las.points))\n",
        "    return counts\n",
        "\n",
        "\n",
        "def print_stats_point_counts(counts: List[int]) -> None:\n",
        "    if not counts:\n",
        "        print(\"No LAS files found for stats.\")\n",
        "        return\n",
        "    total = len(counts)\n",
        "    sorted_counts = sorted(counts)\n",
        "    minimum = sorted_counts[0]\n",
        "    maximum = sorted_counts[-1]\n",
        "    mean = float(np.mean(sorted_counts))\n",
        "    median = float(np.median(sorted_counts))\n",
        "    p25 = float(np.percentile(sorted_counts, 25))\n",
        "    p75 = float(np.percentile(sorted_counts, 75))\n",
        "    print(\"Point counts per cylinder (files):\")\n",
        "    print(\n",
        "        \"  count=\", total,\n",
        "        \"min=\", minimum,\n",
        "        \"p25=\", f\"{p25:.1f}\",\n",
        "        \"median=\", f\"{median:.1f}\",\n",
        "        \"p75=\", f\"{p75:.1f}\",\n",
        "        \"max=\", maximum,\n",
        "        \"mean=\", f\"{mean:.1f}\",\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CylinderDataset(Dataset):\n",
        "    def __init__(\n",
        "        self,\n",
        "        files: List[Tuple[Path, int]],\n",
        "        class_count: int,\n",
        "        points_per_sample: int,\n",
        "        seed: int,\n",
        "    ) -> None:\n",
        "        self.files = files\n",
        "        self.class_count = class_count\n",
        "        self.points_per_sample = points_per_sample\n",
        "        self.random = random.Random(seed)\n",
        "        self._cache: Dict[Path, np.ndarray] = {}\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.files)\n",
        "\n",
        "    def _load_points(self, path: Path) -> np.ndarray:\n",
        "        if path in self._cache:\n",
        "            return self._cache[path]\n",
        "        las = laspy.read(path)\n",
        "        pts = np.stack([las.x, las.y, las.z], axis=1).astype(np.float32)\n",
        "        self._cache[path] = pts\n",
        "        return pts\n",
        "\n",
        "    def _sample_points(self, pts: np.ndarray) -> np.ndarray:\n",
        "        n = pts.shape[0]\n",
        "        if n >= self.points_per_sample:\n",
        "            idx = np.array(self.random.sample(range(n), self.points_per_sample))\n",
        "            return pts[idx]\n",
        "        idx = np.array([self.random.randrange(n) for _ in range(self.points_per_sample)])\n",
        "        return pts[idx]\n",
        "\n",
        "    def __getitem__(self, index: int) -> Tuple[torch.Tensor, int]:\n",
        "        path, label = self.files[index]\n",
        "        pts = self._load_points(path)\n",
        "        sampled = self._sample_points(pts)\n",
        "        centroid = sampled.mean(axis=0, keepdims=True)\n",
        "        centered = sampled - centroid\n",
        "        scale = np.linalg.norm(centered, axis=1).max()\n",
        "        if scale > 0:\n",
        "            centered = centered / scale\n",
        "        tensor = torch.from_numpy(centered.T)\n",
        "        return tensor, label\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PointNetTiny(nn.Module):\n",
        "    def __init__(self, num_classes: int) -> None:\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv1d(3, 64, kernel_size=1)\n",
        "        self.bn1 = nn.BatchNorm1d(64)\n",
        "        self.conv2 = nn.Conv1d(64, 128, kernel_size=1)\n",
        "        self.bn2 = nn.BatchNorm1d(128)\n",
        "        self.conv3 = nn.Conv1d(128, 256, kernel_size=1)\n",
        "        self.bn3 = nn.BatchNorm1d(256)\n",
        "        self.fc1 = nn.Linear(256, 128)\n",
        "        self.bn4 = nn.BatchNorm1d(128)\n",
        "        self.drop1 = nn.Dropout(p=0.3)\n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "        self.bn5 = nn.BatchNorm1d(64)\n",
        "        self.drop2 = nn.Dropout(p=0.3)\n",
        "        self.fc3 = nn.Linear(64, num_classes)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "        x = F.relu(self.bn2(self.conv2(x)))\n",
        "        x = F.relu(self.bn3(self.conv3(x)))\n",
        "        x = torch.max(x, dim=2)[0]\n",
        "        x = F.relu(self.bn4(self.fc1(x)))\n",
        "        x = self.drop1(x)\n",
        "        x = F.relu(self.bn5(self.fc2(x)))\n",
        "        x = self.drop2(x)\n",
        "        logits = self.fc3(x)\n",
        "        return logits\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Split and balance helpers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def split_train_val(files_by_class: Dict[int, List[Path]], split: float, seed: int) -> Tuple[List[Tuple[Path, int]], List[Tuple[Path, int]]]:\n",
        "    rng = random.Random(seed)\n",
        "    train: List[Tuple[Path, int]] = []\n",
        "    val: List[Tuple[Path, int]] = []\n",
        "    for cls, files in files_by_class.items():\n",
        "        shuffled = files[:]\n",
        "        rng.shuffle(shuffled)\n",
        "        n_train = int(len(shuffled) * split)\n",
        "        train.extend((p, cls) for p in shuffled[:n_train])\n",
        "        val.extend((p, cls) for p in shuffled[n_train:])\n",
        "    return train, val\n",
        "\n",
        "\n",
        "def build_balanced_index(samples: List[Tuple[Path, int]], num_classes: int, seed: int) -> List[Tuple[Path, int]]:\n",
        "    rng = random.Random(seed)\n",
        "    per_class: Dict[int, List[Path]] = {c: [] for c in range(num_classes)}\n",
        "    for p, c in samples:\n",
        "        per_class[c].append(p)\n",
        "    class_sizes = {c: len(v) for c, v in per_class.items()}\n",
        "    target = max(class_sizes.values()) if class_sizes else 0\n",
        "    balanced: List[Tuple[Path, int]] = []\n",
        "    for c, files in per_class.items():\n",
        "        if not files:\n",
        "            continue\n",
        "        k = math.ceil(target / len(files))\n",
        "        for _ in range(k):\n",
        "            shuffled = files[:]\n",
        "            rng.shuffle(shuffled)\n",
        "            for p in shuffled:\n",
        "                balanced.append((p, c))\n",
        "    return balanced\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data preparation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Step 1: Scanning LAS cylinders recursively...\")\n",
        "files = find_all_las_files(cfg.data_root)\n",
        "print(\"  Found files:\", len(files))\n",
        "if not files:\n",
        "    raise RuntimeError(f\"No LAS files found in {cfg.data_root}\")\n",
        "\n",
        "print(\"Step 2: Deriving species labels from filenames, selecting target classes, and computing stats...\")\n",
        "species_names_all: List[str] = [extract_species_from_filename(p) for p in files]\n",
        "target_classes_order = [\"aspen\", \"birch\", \"spruce\", \"pine\"]\n",
        "selected: List[Tuple[Path, str]] = [\n",
        "    (p, s) for p, s in zip(files, species_names_all) if s in set(target_classes_order)\n",
        "]\n",
        "files = [p for p, _ in selected]\n",
        "species_names: List[str] = [s for _, s in selected]\n",
        "unique_species = [s for s in target_classes_order if s in set(species_names)]\n",
        "species_to_index: Dict[str, int] = {s: i for i, s in enumerate(unique_species)}\n",
        "print(\"  Target classes (species):\", unique_species)\n",
        "\n",
        "files_by_class: Dict[int, List[Path]] = {i: [] for i in range(len(unique_species))}\n",
        "for p, s in zip(files, species_names):\n",
        "    files_by_class[species_to_index[s]].append(p)\n",
        "\n",
        "print(\"  Samples per class (by file count):\")\n",
        "for s in unique_species:\n",
        "    c = species_to_index[s]\n",
        "    print(\"   \", s, \"->\", len(files_by_class[c]))\n",
        "\n",
        "counts = compute_point_counts(files)\n",
        "print_stats_point_counts(counts)\n",
        "\n",
        "print(\"Step 3: Splitting train/val...\")\n",
        "train_samples, val_samples = split_train_val(files_by_class, cfg.train_split, cfg.seed)\n",
        "print(\"  Train files:\", len(train_samples), \" Val files:\", len(val_samples))\n",
        "\n",
        "print(\"Step 4: Balancing classes by oversampling rarer classes with different point subsets...\")\n",
        "balanced_train = build_balanced_index(train_samples, len(unique_species), cfg.seed)\n",
        "print(\"  Balanced train samples:\", len(balanced_train))\n",
        "\n",
        "num_classes = len(unique_species)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device.type)\n",
        "\n",
        "train_ds = CylinderDataset(balanced_train, num_classes, cfg.points_per_sample, cfg.seed)\n",
        "val_ds = CylinderDataset(val_samples, num_classes, cfg.points_per_sample, cfg.seed)\n",
        "pin = True if device.type == \"cuda\" else False\n",
        "train_loader = DataLoader(train_ds, batch_size=cfg.batch_size, shuffle=True, num_workers=cfg.num_workers, pin_memory=pin)\n",
        "val_loader = DataLoader(val_ds, batch_size=cfg.batch_size, shuffle=False, num_workers=cfg.num_workers, pin_memory=pin)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Training loop with live plots\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_history(history: Dict[str, List[float]]) -> None:\n",
        "    epochs = list(range(1, len(history[\"train_loss\"]) + 1))\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(10, 3))\n",
        "    axes[0].plot(epochs, history[\"train_loss\"], label=\"train_loss\")\n",
        "    axes[0].set_xlabel(\"epoch\")\n",
        "    axes[0].set_ylabel(\"loss\")\n",
        "    axes[0].grid(True, alpha=0.3)\n",
        "    axes[0].legend()\n",
        "\n",
        "    axes[1].plot(epochs, history[\"train_acc\"], label=\"train_acc\")\n",
        "    axes[1].plot(epochs, history[\"val_acc\"], label=\"val_acc\")\n",
        "    axes[1].set_xlabel(\"epoch\")\n",
        "    axes[1].set_ylabel(\"accuracy\")\n",
        "    axes[1].set_ylim(0.0, 1.0)\n",
        "    axes[1].grid(True, alpha=0.3)\n",
        "    axes[1].legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    display(fig)\n",
        "    plt.close(fig)\n",
        "\n",
        "\n",
        "def train_loop(\n",
        "    model: nn.Module,\n",
        "    train_loader: DataLoader,\n",
        "    val_loader: DataLoader,\n",
        "    epochs: int,\n",
        "    device: torch.device,\n",
        "    lr: float,\n",
        "    live_plot: bool,\n",
        ") -> Dict[str, List[float]]:\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    history: Dict[str, List[float]] = {\"train_loss\": [], \"train_acc\": [], \"val_acc\": []}\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for pts, labels in train_loader:\n",
        "            pts = pts.to(device)\n",
        "            labels = labels.to(device)\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "            logits = model(pts)\n",
        "            loss = criterion(logits, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += float(loss.item()) * pts.size(0)\n",
        "            preds = torch.argmax(logits, dim=1)\n",
        "            correct += int((preds == labels).sum().item())\n",
        "            total += int(pts.size(0))\n",
        "        train_loss = running_loss / max(1, total)\n",
        "        train_acc = correct / max(1, total)\n",
        "\n",
        "        model.eval()\n",
        "        val_correct = 0\n",
        "        val_total = 0\n",
        "        with torch.no_grad():\n",
        "            for pts, labels in val_loader:\n",
        "                pts = pts.to(device)\n",
        "                labels = labels.to(device)\n",
        "                logits = model(pts)\n",
        "                preds = torch.argmax(logits, dim=1)\n",
        "                val_correct += int((preds == labels).sum().item())\n",
        "                val_total += int(pts.size(0))\n",
        "        val_acc = val_correct / max(1, val_total)\n",
        "\n",
        "        history[\"train_loss\"].append(train_loss)\n",
        "        history[\"train_acc\"].append(train_acc)\n",
        "        history[\"val_acc\"].append(val_acc)\n",
        "\n",
        "        print(f\"Epoch {epoch}/{epochs} - train_loss={train_loss:.4f} train_acc={train_acc:.3f} val_acc={val_acc:.3f}\")\n",
        "        if live_plot:\n",
        "            clear_output(wait=True)\n",
        "            print(f\"Epoch {epoch}/{epochs} - train_loss={train_loss:.4f} train_acc={train_acc:.3f} val_acc={val_acc:.3f}\")\n",
        "            plot_history(history)\n",
        "\n",
        "    return history\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Train\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = PointNetTiny(num_classes=num_classes).to(device)\n",
        "print(\"Training PointNet...\")\n",
        "history = train_loop(\n",
        "    model=model,\n",
        "    train_loader=train_loader,\n",
        "    val_loader=val_loader,\n",
        "    epochs=cfg.epochs,\n",
        "    device=device,\n",
        "    lr=cfg.learning_rate,\n",
        "    live_plot=cfg.live_plot,\n",
        ")\n",
        "\n",
        "# Final plots (in case live plotting was disabled)\n",
        "plot_history(history)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Save artifacts\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_path = cfg.output_dir / \"pointnet_tiny.pth\"\n",
        "torch.save(model.state_dict(), model_path)\n",
        "with (cfg.output_dir / \"species_index.json\").open(\"w\", encoding=\"utf-8\") as fh:\n",
        "    json.dump({\"classes\": unique_species}, fh, ensure_ascii=False, indent=2)\n",
        "print(\"Saved model to:\", model_path)\n",
        "print(\"Saved class mapping to:\", cfg.output_dir / \"species_index.json\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
